{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import imageio as img\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #for training on gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting variables for the directory to the data\n",
    "todata = \"/home/abdullah/Desktop/Abdullah/LUMS/Senior/Sproj/ImgPreProcessing/Channel/\"\n",
    "moredata = \"/home/abdullah/Desktop/Abdullah/LUMS/Senior/Sproj/ImgPreProcessing/AmplifiedDataset\"\n",
    "\n",
    "\n",
    "training_iters = 10 \n",
    "learning_rate = 0.0001 \n",
    "batch_size = 128\n",
    "\n",
    "# data input (img shape: 28*28)\n",
    "n_input = 28\n",
    "\n",
    "# total classes (0-9 digits)\n",
    "n_classes = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data labales matrix\n",
      "['A' 'A' 'A' ... 'Z' 'Z' 'Z'] <U1 (3844,)\n",
      "\n",
      "Data matrix\n",
      "float32 (3844, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# saving current directory\n",
    "cur = os.getcwd()\n",
    "\n",
    "# changing directoy to data set\n",
    "os.chdir(todata)\n",
    "# getting all the folder names\n",
    "nmes = os.listdir(\".\")\n",
    "nmes.sort()\n",
    "\n",
    "# reading all the data into labels and data numpy arrays\n",
    "labels = []\n",
    "data = []\n",
    "for letr in nmes:\n",
    "    for file in os.listdir(todata+letr):\n",
    "        labels.append(letr)\n",
    "        f = img.imread(todata + letr + \"/\"+ file)\n",
    "        data.append(f)\n",
    "\n",
    "        # change back directory\n",
    "os.chdir(cur)\n",
    "\n",
    "print(\"\\nData labales matrix\")\n",
    "labels = np.array(labels)\n",
    "print(labels, labels.dtype, labels.shape)\n",
    "\n",
    "# diving by 255 to get clamp values between zero and one\n",
    "print(\"\\nData matrix\")\n",
    "data = np.array(data, np.float32)/255\n",
    "print(data.dtype, data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3844, 26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdullah/.local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# converting labels into onehot encodings\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(labels)\n",
    "# print(integer_encoded)\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that splits up the data\n",
    "\n",
    "def train_val_split(data, labels, keep):\n",
    "  \n",
    "\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    #split the data\n",
    "    number_of_validation_points = data.shape[0]\n",
    "\n",
    "    split = int(number_of_validation_points*keep)\n",
    "\n",
    "    # getting curent state so shuffle is the same for both arrays\n",
    "    curState = np.random.get_state()\n",
    "    np.random.shuffle(data)\n",
    "    # setting the state\n",
    "    np.random.set_state(curState)\n",
    "    np.random.shuffle(labels)\n",
    "\n",
    "\n",
    "\n",
    "    #images are unflattened\n",
    "    temp_val = data[split:]\n",
    "    val_label = labels[split:]\n",
    "    temp_train = data[:split]\n",
    "    train_label = labels[:split]\n",
    "\n",
    "    val = temp_val\n",
    "    train = temp_train\n",
    "    \n",
    "    train = np.array(train)\n",
    "    val = np.array(val)\n",
    "\n",
    "    print(\"The splits are: \")\n",
    "    print(train.shape)   \n",
    "    print(train_label.shape)\n",
    "    print(val.shape) \n",
    "    print(val_label.shape)\n",
    "\n",
    "    return train.T, train_label, val.T, val_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The splits are: \n",
      "(3075, 28, 28)\n",
      "(3075, 26)\n",
      "(769, 28, 28)\n",
      "(769, 26)\n"
     ]
    }
   ],
   "source": [
    "# splitting up the data\n",
    "tr, tr_l, vl, vl_l = train_val_split(data, onehot_encoded, 0.8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3075, 28, 28, 1), (769, 28, 28, 1), (3075, 26), (769, 26))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape training and testing image\n",
    "train_X = tr.reshape(-1, n_input, n_input, 1)\n",
    "test_X = vl.reshape(-1,n_input,n_input, 1)\n",
    "\n",
    "\n",
    "\n",
    "train_y = tr_l\n",
    "test_y = vl_l\n",
    "\n",
    "train_X.shape, test_X.shape, train_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow variables will be assigned inside the functions\n",
    "\n",
    "# simple convolution layer\n",
    "def conv_layer(inp, ch_in, ch_out, name=\"conv\", strides=1, k=2):\n",
    "    # giving the same naming structure for graph in tensorboard\n",
    "    with tf.name_scope(name):\n",
    "        # the first two are kernel dimen then its the input depth and \n",
    "        # then the number of filters you want ie output depth\n",
    "        w = tf.Variable(tf.truncated_normal([5, 5, ch_in, ch_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[ch_out]), name=\"B\")\n",
    "\n",
    "        # applying the kernel\n",
    "        conv = tf.nn.conv2d(inp, w, strides=[1, strides, strides,1], padding=\"SAME\")\n",
    "\n",
    "        # activation\n",
    "        act = tf.nn.relu(conv + b)\n",
    "\n",
    "        return tf.nn.max_pool(act, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "\n",
    "# fully connected layer\n",
    "def fc_layer(inp, ch_in, ch_out, name=\"fulluConnected\"):\n",
    "    # giving the same naming structure for graph in tensorboard\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([ch_in, ch_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[ch_out]), name=\"B\")\n",
    "\n",
    "        # linear operation and activation\n",
    "        act = tf.nn.relu(tf.matmul(inp, w) + b)\n",
    "\n",
    "        return act\n",
    "\n",
    "\n",
    "def maxpool_layer(x, name=\"maxpool\", k=2):\n",
    "    # giving the same naming structure for graph in tensorboard\n",
    "    with tf.name_scope(name):\n",
    "        return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the whole compuational graph/network\n",
    "def conv_net(x):  \n",
    "\n",
    "    # here we call the convolution layer we had defined above and pass the input image x\n",
    "    # since image is grascale input channel = 1 and output we specified to 32 kernels\n",
    "    conv1 = conv_layer(x, 1, 32, \"conv1\")\n",
    "    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 14*14 matrix.\n",
    "#     pool1 = maxpool_layer(conv1, \"maxpool1\", 2)\n",
    "\n",
    "\n",
    "    # here we call the convolution layer we had defined above.\n",
    "    conv2 = conv_layer(conv1, 32, 64, \"conv2\")\n",
    "    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 7*7 matrix.\n",
    "#     pool2 = maxpool_layer(conv2, \"maxpool2\", 2)\n",
    "\n",
    "    conv3 = conv_layer(conv2, 64, 128, \"conv3\")\n",
    "    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 4*4.\n",
    "#     pool3 = maxpool_layer(conv3, \"maxpool3\", 2)\n",
    "\n",
    "\n",
    "    # flatten the ouput of pool for fully connected\n",
    "    # Reshape output to fit fully connected layer input\n",
    "    flat = tf.reshape(conv3, [-1, math.ceil(n_classes/8)*math.ceil(n_classes/8)*128])\n",
    "\n",
    "    # Fully connected layer\n",
    "    fc1 = fc_layer(flat, math.ceil(n_classes/8)*math.ceil(n_classes/8)*128, 1024, \"fc1\")\n",
    "\n",
    "    # Output, class prediction\n",
    "    # finally we multiply the fully connected layer with the weights and add a bias term. \n",
    "    out = fc_layer(fc1, 1024, n_classes,\"fc2\")\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Loss= 3.258096, Training Accuracy= 0.04688\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.04421\n",
      "Iter 1, Loss= 3.258096, Training Accuracy= 0.04688\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.04421\n",
      "Iter 2, Loss= 3.258096, Training Accuracy= 0.04688\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.04421\n",
      "Iter 3, Loss= 3.258096, Training Accuracy= 0.04688\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.04421\n",
      "Iter 4, Loss= 3.258096, Training Accuracy= 0.04688\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.04421\n",
      "Iter 5, Loss= 3.258096, Training Accuracy= 0.04688\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.04421\n",
      "Iter 6, Loss= 3.258096, Training Accuracy= 0.04688\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.04421\n",
      "Iter 7, Loss= 3.258096, Training Accuracy= 0.04688\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.04421\n",
      "Iter 8, Loss= 3.258096, Training Accuracy= 0.04688\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.04421\n",
      "Iter 9, Loss= 3.258096, Training Accuracy= 0.04688\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.04421\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()# telling the session to initialize the variables\n",
    "\n",
    "\n",
    "# input dimen and output dimen\n",
    "x = tf.placeholder(\"float\", [None, n_input,n_input, 1], name=\"x\")\n",
    "y = tf.placeholder(\"float\", [None, n_classes], name = \"labels\")\n",
    "\n",
    "# storing the graph in the variable pred\n",
    "pred = conv_net(x)\n",
    "\n",
    "# cost fucntion for the graph\n",
    "with tf.name_scope(\"entropy\"):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n",
    "\n",
    "# optiizr for the network\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    #Here you check whether the index of the maximum value of the predicted image is equal to the actual labelled image. and both will be a column vector.\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "\n",
    "    #calculate accuracy across all the given images and average them out. \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) \n",
    "\n",
    "# storing the values in lists\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "summary_writer = tf.summary.FileWriter('./Output')\n",
    "summary_writer.add_graph(sess.graph)\n",
    "for i in range(training_iters):\n",
    "    for batch in range(len(train_X)//batch_size):\n",
    "        batch_x = train_X[batch*batch_size:min((batch+1)*batch_size,len(train_X))]\n",
    "        batch_y = train_y[batch*batch_size:min((batch+1)*batch_size,len(train_y))]    \n",
    "        # Run optimization op (backprop).\n",
    "        # Calculate batch loss and accuracy\n",
    "        opt = sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "    print(\"Iter \" + str(i) + \", Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for all test images\n",
    "    test_acc,valid_loss = sess.run([accuracy,cost], feed_dict={x: test_X,y : test_y})\n",
    "    train_loss.append(loss)\n",
    "    test_loss.append(valid_loss)\n",
    "    train_accuracy.append(acc)\n",
    "    test_accuracy.append(test_acc)\n",
    "    print(\"Testing Accuracy:\",\"{:.5f}\".format(test_acc))\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
